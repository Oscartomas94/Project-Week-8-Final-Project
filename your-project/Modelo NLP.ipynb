{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empezar modelo NLP - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases1 = pd.read_csv(\"../Dataset/Suicidal phrases/dataset2.csv\")\n",
    "phrases = pd.read_csv(\"../Dataset/Suicidal phrases/dataset1.csv\") #no hay NaN. Target 0: depresión. 1: bien. 11434 rows\n",
    "phrases3 = pd.read_csv(\"../Dataset/Suicidal phrases/dataset3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>289612631038361600</td>\n",
       "      <td>Argh.. I hate my life</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>289612727654170624</td>\n",
       "      <td>I'm good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>289612736063758337</td>\n",
       "      <td>Enjoy my life</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>289612773716008960</td>\n",
       "      <td>RT : I do what I want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>289612819807211520</td>\n",
       "      <td>My life is just a series of unfortunate fucked...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11429</th>\n",
       "      <td>285626150695337984</td>\n",
       "      <td>ajhg;dfakjh;ajh;a i'm so disappointed in mysel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11430</th>\n",
       "      <td>285626247164342272</td>\n",
       "      <td>RT : I suck at life</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11431</th>\n",
       "      <td>285626272330178560</td>\n",
       "      <td>My Life Is The Shit I Guess That This Was Mean...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11432</th>\n",
       "      <td>285626292177612801</td>\n",
       "      <td>I hate myself</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11433</th>\n",
       "      <td>285626301627383808</td>\n",
       "      <td>I feel depressed,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11434 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Id                                               Text  \\\n",
       "0      289612631038361600                              Argh.. I hate my life   \n",
       "1      289612727654170624                                           I'm good   \n",
       "2      289612736063758337                                      Enjoy my life   \n",
       "3      289612773716008960                              RT : I do what I want   \n",
       "4      289612819807211520  My life is just a series of unfortunate fucked...   \n",
       "...                   ...                                                ...   \n",
       "11429  285626150695337984  ajhg;dfakjh;ajh;a i'm so disappointed in mysel...   \n",
       "11430  285626247164342272                                RT : I suck at life   \n",
       "11431  285626272330178560  My Life Is The Shit I Guess That This Was Mean...   \n",
       "11432  285626292177612801                                      I hate myself   \n",
       "11433  285626301627383808                                  I feel depressed,   \n",
       "\n",
       "       Target  \n",
       "0           0  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           0  \n",
       "...       ...  \n",
       "11429       0  \n",
       "11430       0  \n",
       "11431       0  \n",
       "11432       0  \n",
       "11433       0  \n",
       "\n",
       "[11434 rows x 3 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    Argh.. I hate my life\n",
       "1                                                 I'm good\n",
       "2                                            Enjoy my life\n",
       "3                                    RT : I do what I want\n",
       "4        My life is just a series of unfortunate fucked...\n",
       "                               ...                        \n",
       "11429    ajhg;dfakjh;ajh;a i'm so disappointed in mysel...\n",
       "11430                                  RT : I suck at life\n",
       "11431    My Life Is The Shit I Guess That This Was Mean...\n",
       "11432                                        I hate myself\n",
       "11433                                    I feel depressed,\n",
       "Name: Text, Length: 11434, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create the DataFrame\n",
    "\n",
    "phrases_t = phrases['Text']\n",
    "phrases_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Dataset and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_regex(tweet, regexp):\n",
    "        return re.sub(regexp, '', tweet)\n",
    "\n",
    "def remove_special_char(tweet):\n",
    "    return re.sub(r\"[^a-zA-Z0-9 ]\", \" \", tweet) #add space placeholder\n",
    "\n",
    "def remove_numbers(tweet):\n",
    "    return remove_by_regex(tweet, re.compile(r\"[1234567890]\"))\n",
    "\n",
    "def clean_up(tweet):\n",
    "    tweet = remove_numbers(tweet)\n",
    "    tweet = remove_special_char(tweet)\n",
    "    return tweet.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    argh   i hate my life\n",
       "1                                                 i m good\n",
       "2                                            enjoy my life\n",
       "3                                    rt   i do what i want\n",
       "4        my life is just a series of unfortunate fucked...\n",
       "                               ...                        \n",
       "11429    ajhg dfakjh ajh a i m so disappointed in mysel...\n",
       "11430                                  rt   i suck at life\n",
       "11431    my life is the shit i guess that this was mean...\n",
       "11432                                        i hate myself\n",
       "11433                                     i feel depressed\n",
       "Name: Text, Length: 11434, dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_text = phrases_t.apply(clean_up)\n",
    "phrases_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize & Lemmatize (lemma to do later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = phrases_text.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porter_text = tokenized.apply(PorterStemmer.stem(tokenized))\n",
    "\n",
    "# lemmatizer = tokenized.apply(WordNetLemmatizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And we are going to create a function that removes stopwords\n",
    "\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    return [x for x in tweet if x not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tokenized.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                       [argh, hate, life]\n",
       "1                                                   [good]\n",
       "2                                            [enjoy, life]\n",
       "3                                               [rt, want]\n",
       "4        [life, series, unfortunate, fucked, events, ma...\n",
       "                               ...                        \n",
       "11429               [ajhg, dfakjh, ajh, disappointed, orz]\n",
       "11430                                     [rt, suck, life]\n",
       "11431                           [life, shit, guess, meant]\n",
       "11432                                               [hate]\n",
       "11433                                    [feel, depressed]\n",
       "Name: Text, Length: 11434, dtype: object"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling feature lists of words from positive reviews and words from the negative tweets \n",
    "#to hopefully see trends in specific types of words in positive or negative tweets.\n",
    "\n",
    "all_words = []\n",
    "for index, value in f.iteritems():\n",
    "    if value not in all_words:\n",
    "        all_words += value\n",
    "\n",
    "word_features = [x[0] for x in nltk.FreqDist(all_words).most_common(3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life',\n",
       " 'rt',\n",
       " 'good',\n",
       " 'happy',\n",
       " 'love',\n",
       " 'hate',\n",
       " 'want',\n",
       " 'god',\n",
       " 'blessed',\n",
       " 'feel',\n",
       " 'fuck',\n",
       " 'sad',\n",
       " 'blessing',\n",
       " 'thanks',\n",
       " 'deserve',\n",
       " 'best',\n",
       " 'great',\n",
       " 'everything',\n",
       " 'bad',\n",
       " 'like',\n",
       " 'friends',\n",
       " 'ever',\n",
       " 'really',\n",
       " 'fine',\n",
       " 'shit',\n",
       " 'get',\n",
       " 'know',\n",
       " 'boring',\n",
       " 'worst',\n",
       " 'fucking',\n",
       " 'much',\n",
       " 'lol',\n",
       " 'always',\n",
       " 'tripping',\n",
       " 'need',\n",
       " 'proud',\n",
       " 'awesome',\n",
       " 'meekmill',\n",
       " 'person',\n",
       " 'im',\n",
       " 'sucks',\n",
       " 'never',\n",
       " 'perfect',\n",
       " 'got',\n",
       " 'girl',\n",
       " 'people',\n",
       " 'think',\n",
       " 'thank',\n",
       " 'stressed',\n",
       " 'change',\n",
       " 'use',\n",
       " 'man',\n",
       " 'depressed',\n",
       " 'live',\n",
       " 'one',\n",
       " 'ugly',\n",
       " 'things',\n",
       " 'feeling',\n",
       " 'beautiful',\n",
       " 'amazing',\n",
       " 'die',\n",
       " 'thankful',\n",
       " 'suck',\n",
       " 'nothing',\n",
       " 'damn',\n",
       " 'mood',\n",
       " 'pretty',\n",
       " 'dont',\n",
       " 'could',\n",
       " 'time',\n",
       " 'way',\n",
       " 'world',\n",
       " 'omg',\n",
       " 'reiatable',\n",
       " 'complete',\n",
       " 'wish',\n",
       " 'yeah',\n",
       " 'true',\n",
       " 'oh',\n",
       " 'day',\n",
       " 'haha',\n",
       " 'yes',\n",
       " 'still',\n",
       " 'make',\n",
       " 'headache',\n",
       " 'cause',\n",
       " 'say',\n",
       " 'loving',\n",
       " 'living',\n",
       " 'right',\n",
       " 'swear',\n",
       " 'single',\n",
       " 'truly',\n",
       " 'everyone',\n",
       " 'thing',\n",
       " 'every',\n",
       " 'sick',\n",
       " 'actually',\n",
       " 'even',\n",
       " 'honestly',\n",
       " 'see',\n",
       " 'anything',\n",
       " 'boy',\n",
       " 'going',\n",
       " 'lucky',\n",
       " 'come',\n",
       " 'co',\n",
       " 'better',\n",
       " 'http',\n",
       " 'joke',\n",
       " 'guy',\n",
       " 'nah',\n",
       " 'boyfriend',\n",
       " 'stupid',\n",
       " 'hard',\n",
       " 'family',\n",
       " 'seriously',\n",
       " 'bless',\n",
       " 'tired',\n",
       " 'satisfied',\n",
       " 'dreams',\n",
       " 'wow',\n",
       " 'na',\n",
       " 'go',\n",
       " 'everyday',\n",
       " 'well',\n",
       " 'fun',\n",
       " 'okay',\n",
       " 'luck',\n",
       " 'try',\n",
       " 'terrible',\n",
       " 'u',\n",
       " 'friend',\n",
       " 'miserable',\n",
       " 'horrible',\n",
       " 'made',\n",
       " 'content',\n",
       " 'real',\n",
       " 'year',\n",
       " 'matter',\n",
       " 'getting',\n",
       " 'sorry',\n",
       " 'jealous',\n",
       " 'enjoy',\n",
       " 'new',\n",
       " 'unhappy',\n",
       " 'bitch',\n",
       " 'enough',\n",
       " 'lmao',\n",
       " 'ugh',\n",
       " 'full',\n",
       " 'lord',\n",
       " 'back',\n",
       " 'stays',\n",
       " 'keep',\n",
       " 'whole',\n",
       " 'though',\n",
       " 'top',\n",
       " 'forever',\n",
       " 'dream',\n",
       " 'wanted',\n",
       " 'many',\n",
       " 'someone',\n",
       " 'ha',\n",
       " 'makes',\n",
       " 'x',\n",
       " 'big',\n",
       " 'happier',\n",
       " 'guess',\n",
       " 'fuckin',\n",
       " 'something',\n",
       " 'damnitstrue',\n",
       " 'disappointed',\n",
       " 'end',\n",
       " 'happiness',\n",
       " 'anymore',\n",
       " 'literally',\n",
       " 'fabulous',\n",
       " 'girlfriend',\n",
       " 'wonderful',\n",
       " 'wan',\n",
       " 'done',\n",
       " 'crazy',\n",
       " 'ask',\n",
       " 'wrong',\n",
       " 'find',\n",
       " 'different',\n",
       " 'might',\n",
       " 'glad',\n",
       " 'believe',\n",
       " 'look',\n",
       " 'another',\n",
       " 'tries',\n",
       " 'night',\n",
       " 'little',\n",
       " 'tho',\n",
       " 'shitty',\n",
       " 'already',\n",
       " 'n',\n",
       " 'lie',\n",
       " 'sweet',\n",
       " 'fucked',\n",
       " 'failure',\n",
       " 'felt',\n",
       " 'allah',\n",
       " 'extremely',\n",
       " 'social',\n",
       " 'mywishin',\n",
       " 'positive',\n",
       " 'disappointment',\n",
       " 'mom',\n",
       " 'care',\n",
       " 'thats',\n",
       " 'cuz',\n",
       " 'heart',\n",
       " 'times',\n",
       " 'tedsposts',\n",
       " 'perfectly',\n",
       " 'easily',\n",
       " 'alive',\n",
       " 'would',\n",
       " 'alone',\n",
       " 'factsforgiris',\n",
       " 'mess',\n",
       " 'ass',\n",
       " 'hahaha',\n",
       " 'please',\n",
       " 'crying',\n",
       " 'check',\n",
       " 'complain',\n",
       " 'falling',\n",
       " 'dear',\n",
       " 'yea',\n",
       " 'enjoying',\n",
       " 'else',\n",
       " 'bored',\n",
       " 'anyone',\n",
       " 'head',\n",
       " 'soo',\n",
       " 'sleep',\n",
       " 'amp',\n",
       " 'idk',\n",
       " 'without',\n",
       " 'sigh',\n",
       " 'whatever',\n",
       " 'away',\n",
       " 'tell',\n",
       " 'c',\n",
       " 'none',\n",
       " 'cry',\n",
       " 'found',\n",
       " 'smile',\n",
       " 'stop',\n",
       " 'amen',\n",
       " 'pathetic',\n",
       " 'point',\n",
       " 'give',\n",
       " 'let',\n",
       " 'long',\n",
       " 'beyond',\n",
       " 'said',\n",
       " 'rn',\n",
       " 'sure',\n",
       " 'depression',\n",
       " 'happens',\n",
       " 'buy',\n",
       " 'far',\n",
       " 'sometimes',\n",
       " 'tbh',\n",
       " 'complicated',\n",
       " 'yay',\n",
       " 'days',\n",
       " 'ok',\n",
       " 'job',\n",
       " 'ah',\n",
       " 'nobody',\n",
       " 'guys',\n",
       " 'breathing',\n",
       " 'w',\n",
       " 'saddest',\n",
       " 'talk',\n",
       " 'mad',\n",
       " 'face',\n",
       " 'gon',\n",
       " 'kinda',\n",
       " 'today',\n",
       " 'jesus',\n",
       " 'tweet',\n",
       " 'place',\n",
       " 'may',\n",
       " 'hey',\n",
       " 'wake',\n",
       " 'suicide',\n",
       " 'lmfao',\n",
       " 'fart',\n",
       " 'run',\n",
       " 'start',\n",
       " 'finally',\n",
       " 'nope',\n",
       " 'hell',\n",
       " 'smh',\n",
       " 'k',\n",
       " 'b',\n",
       " 'happen',\n",
       " 'fat',\n",
       " 'pain',\n",
       " 'maybe',\n",
       " 'lot',\n",
       " 'realize',\n",
       " 'words',\n",
       " 'funniest',\n",
       " 'morning',\n",
       " 'p',\n",
       " 'wishes',\n",
       " 'free',\n",
       " 'young',\n",
       " 'parents',\n",
       " 'since',\n",
       " 'laugh',\n",
       " 'reason',\n",
       " 'thinking',\n",
       " 'dnt',\n",
       " 'cant',\n",
       " 'old',\n",
       " 'omfg',\n",
       " 'feels',\n",
       " 'take',\n",
       " 'coming',\n",
       " 'happened',\n",
       " 'sunshine',\n",
       " 'useless',\n",
       " 'everybody',\n",
       " 'looking',\n",
       " 'awkward',\n",
       " 'dude',\n",
       " 'high',\n",
       " 'story',\n",
       " 'wait',\n",
       " 'sister',\n",
       " 'miss',\n",
       " 'keeps',\n",
       " 'forgive',\n",
       " 'watching',\n",
       " 'dad',\n",
       " 'dying',\n",
       " 'boredom',\n",
       " 'least',\n",
       " 'woke',\n",
       " 'places',\n",
       " 'bed',\n",
       " 'simple',\n",
       " 'school',\n",
       " 'hope',\n",
       " 'comfy',\n",
       " 'stay',\n",
       " 'call',\n",
       " 'years',\n",
       " 'absolutely',\n",
       " 'hahahaha',\n",
       " 'gosh',\n",
       " 'nice',\n",
       " 'super',\n",
       " 'feelin',\n",
       " 'plans',\n",
       " 'mean',\n",
       " 'hurt',\n",
       " 'seeing',\n",
       " 'uncomfortable',\n",
       " 'used',\n",
       " 'making',\n",
       " 'grateful',\n",
       " 'describe',\n",
       " 'baby',\n",
       " 'ya',\n",
       " 'exciting',\n",
       " 'gone',\n",
       " 'home',\n",
       " 'xd',\n",
       " 'alright',\n",
       " 'lt',\n",
       " 'memory',\n",
       " 'e',\n",
       " 'thinks',\n",
       " 'break',\n",
       " 'bc',\n",
       " 'awful',\n",
       " 'lame',\n",
       " 'holy',\n",
       " 'camper',\n",
       " 'genuinely',\n",
       " 'nigga',\n",
       " 'kill',\n",
       " 'together',\n",
       " 'regrets',\n",
       " 'ways',\n",
       " 'ones',\n",
       " 'trying',\n",
       " 'either',\n",
       " 'stress',\n",
       " 'totally',\n",
       " 'uh',\n",
       " 'entire',\n",
       " 'depressing',\n",
       " 'probably',\n",
       " 'last',\n",
       " 'fact',\n",
       " 'dailyteenwords',\n",
       " 'moment',\n",
       " 'knows',\n",
       " 'also',\n",
       " 'money',\n",
       " 'kid',\n",
       " 'peace',\n",
       " 'blessings',\n",
       " 'plan',\n",
       " 'aint',\n",
       " 'help',\n",
       " 'gets',\n",
       " 'pocket',\n",
       " 'misery',\n",
       " 'hi',\n",
       " 'heartstagram',\n",
       " 'rich',\n",
       " 'goes',\n",
       " 'dead',\n",
       " 'officially',\n",
       " 'fml',\n",
       " 'met',\n",
       " 'mistakes',\n",
       " 'eyes',\n",
       " 'yup',\n",
       " 'trust',\n",
       " 'r',\n",
       " 'around',\n",
       " 'quite',\n",
       " 'worse',\n",
       " 'situation',\n",
       " 'regret',\n",
       " 'relationship',\n",
       " 'empty',\n",
       " 'work',\n",
       " 'goodnight',\n",
       " 'becoming',\n",
       " 'alot',\n",
       " 'child',\n",
       " 'upset',\n",
       " 'obviously',\n",
       " 'play',\n",
       " 'thanking',\n",
       " 'bestest',\n",
       " 'jus',\n",
       " 'bye',\n",
       " 'g',\n",
       " 'struggle',\n",
       " 'lonely',\n",
       " 'put',\n",
       " 'completely',\n",
       " 'idc',\n",
       " 'fullest',\n",
       " 'tf',\n",
       " 'less',\n",
       " 'naw',\n",
       " 'leave',\n",
       " 'went',\n",
       " 'ruined',\n",
       " 'gt',\n",
       " 'laughing',\n",
       " 'twitter',\n",
       " 'almost',\n",
       " 'show',\n",
       " 'thoughts',\n",
       " 'emotional',\n",
       " 'gift',\n",
       " 'da',\n",
       " 'knowing',\n",
       " 'wouldnt',\n",
       " 'ache',\n",
       " 'freaking',\n",
       " 'bro',\n",
       " 'brother',\n",
       " 'apart',\n",
       " 'worries',\n",
       " 'bullshit',\n",
       " 'mind',\n",
       " 'worthless',\n",
       " 'born',\n",
       " 'called',\n",
       " 'giving',\n",
       " 'ppl',\n",
       " 'ur',\n",
       " 'problems',\n",
       " 'basically',\n",
       " 'realized',\n",
       " 'mine',\n",
       " 'funny',\n",
       " 'currently',\n",
       " 'wtf',\n",
       " 'aw',\n",
       " 'explain',\n",
       " 'kind',\n",
       " 'starting',\n",
       " 'bout',\n",
       " 'lil',\n",
       " 'us',\n",
       " 'healthy',\n",
       " 'disgusted',\n",
       " 'shut',\n",
       " 'college',\n",
       " 'everytime',\n",
       " 'simply',\n",
       " 'prefer',\n",
       " 'bit',\n",
       " 'hates',\n",
       " 'thought',\n",
       " 'inside',\n",
       " 'decisions',\n",
       " 'ready',\n",
       " 'wife',\n",
       " 'wit',\n",
       " 'music',\n",
       " 'taking',\n",
       " 'daughter',\n",
       " 'low',\n",
       " 'needs',\n",
       " 'easy',\n",
       " 'comfortable',\n",
       " 'bring',\n",
       " 'v',\n",
       " 'jk',\n",
       " 'left',\n",
       " 'insecure',\n",
       " 'cramps',\n",
       " 'triple',\n",
       " 'anxiety',\n",
       " 'exactly',\n",
       " 'doe',\n",
       " 'sitting',\n",
       " 'trade',\n",
       " 'brilliant',\n",
       " 'drama',\n",
       " 'niggas',\n",
       " 'ill',\n",
       " 'yo',\n",
       " 'spoiled',\n",
       " 'f',\n",
       " 'changed',\n",
       " 'lost',\n",
       " 'lead',\n",
       " 'trillassblasian',\n",
       " 'hurting',\n",
       " 'greatest',\n",
       " 'phone',\n",
       " 'hole',\n",
       " 'memories',\n",
       " 'lmfaoo',\n",
       " 'fix',\n",
       " 'series',\n",
       " 'understand',\n",
       " 'weird',\n",
       " 'next',\n",
       " 'lifes',\n",
       " 'hopes',\n",
       " 'woman',\n",
       " 'smiling',\n",
       " 'meant',\n",
       " 'hilarious',\n",
       " 'paranoid',\n",
       " 'remember',\n",
       " 'babe',\n",
       " 'negative',\n",
       " 'given',\n",
       " 'turned',\n",
       " 'song',\n",
       " 'stuff',\n",
       " 'name',\n",
       " 'scared',\n",
       " 'ikr',\n",
       " 'shoot',\n",
       " 'happening',\n",
       " 'cunt',\n",
       " 'blasting',\n",
       " 'campus',\n",
       " 'nd',\n",
       " 'incomplete',\n",
       " 'existence',\n",
       " 'everywhere',\n",
       " 'angry',\n",
       " 'perfection',\n",
       " 'style',\n",
       " 'possible',\n",
       " 'seem',\n",
       " 'energy',\n",
       " 'working',\n",
       " 'cos',\n",
       " 'moments',\n",
       " 'turning',\n",
       " 'yet',\n",
       " 'gorgeous',\n",
       " 'incredibly',\n",
       " 'gf',\n",
       " 'appreciate',\n",
       " 'atm',\n",
       " 'hating',\n",
       " 'planned',\n",
       " 'livin',\n",
       " 'needed',\n",
       " 'chance',\n",
       " 'apparently',\n",
       " 'side',\n",
       " 'clearly',\n",
       " 'xo',\n",
       " 'ive',\n",
       " 'worry',\n",
       " 'general',\n",
       " 'welcome',\n",
       " 'issues',\n",
       " 'lovely',\n",
       " 'shitfest',\n",
       " 'future',\n",
       " 'lived',\n",
       " 'victorious',\n",
       " 'sadness',\n",
       " 'usomahiya',\n",
       " 'mother',\n",
       " 'pleased',\n",
       " 'rayiopez',\n",
       " 'argh',\n",
       " 'unfortunate',\n",
       " 'become',\n",
       " 'past',\n",
       " 'black',\n",
       " 'ups',\n",
       " 'downs',\n",
       " 'loves',\n",
       " 'huh',\n",
       " 'course',\n",
       " 'highly',\n",
       " 'dey',\n",
       " 'actual',\n",
       " 'biggest',\n",
       " 'must',\n",
       " 'humbled',\n",
       " 'hungry',\n",
       " 'mama',\n",
       " 'hours',\n",
       " 'lovemylife',\n",
       " 'un',\n",
       " 'generally',\n",
       " 'daily',\n",
       " 'refuse',\n",
       " 'gga',\n",
       " 'goals',\n",
       " 'able',\n",
       " 'works',\n",
       " 'minus',\n",
       " 'screw',\n",
       " 'blah',\n",
       " 'fast',\n",
       " 'idea',\n",
       " 'nvm',\n",
       " 'sooo',\n",
       " 'cares',\n",
       " 'rather',\n",
       " 'bunny',\n",
       " 'short',\n",
       " 'embarrassing',\n",
       " 'ntamyra',\n",
       " 'convince',\n",
       " 'white',\n",
       " 'cus',\n",
       " 'tears',\n",
       " 'direction',\n",
       " 'sial',\n",
       " 'ridiculous',\n",
       " 'hahah',\n",
       " 'thx',\n",
       " 'house',\n",
       " 'scream',\n",
       " 'soooo',\n",
       " 'waking',\n",
       " 'drunk',\n",
       " 'hm',\n",
       " 'second',\n",
       " 'changes',\n",
       " 'xoxo',\n",
       " 'seems',\n",
       " 'eat',\n",
       " 'wonder',\n",
       " 'motivation',\n",
       " 'hurts',\n",
       " 'attitude',\n",
       " 'brothers',\n",
       " 'button',\n",
       " 'duh',\n",
       " 'dull',\n",
       " 'disgusting',\n",
       " 'cough',\n",
       " 'listening',\n",
       " 'mi',\n",
       " 'annoyed',\n",
       " 'fr',\n",
       " 'dawg',\n",
       " 'kno',\n",
       " 'dang',\n",
       " 'freakin',\n",
       " 'body',\n",
       " 'sing',\n",
       " 'official',\n",
       " 'fucken',\n",
       " 'movie',\n",
       " 'excuse',\n",
       " 'system',\n",
       " 'fantastic',\n",
       " 'excited',\n",
       " 'bestfriend',\n",
       " 'joy',\n",
       " 'nahh',\n",
       " 'adamtodd',\n",
       " 'legit',\n",
       " 'wizkhalifa',\n",
       " 'amaaaazing',\n",
       " 'patience',\n",
       " 'events',\n",
       " 'open',\n",
       " 'broken',\n",
       " 'attract',\n",
       " 'hahahah',\n",
       " 'alcohol',\n",
       " 'two',\n",
       " 'children',\n",
       " 'laid',\n",
       " 'outta',\n",
       " 'deal',\n",
       " 'busy',\n",
       " 'endless',\n",
       " 'theres',\n",
       " 'favored',\n",
       " 'says',\n",
       " 'late',\n",
       " 'complaints',\n",
       " 'ta',\n",
       " 'j',\n",
       " 'moods',\n",
       " 'buried',\n",
       " 'piece',\n",
       " 'somewhere',\n",
       " 'shoppin',\n",
       " 'aunt',\n",
       " 'sex',\n",
       " 'self',\n",
       " 'continues',\n",
       " 'bastard',\n",
       " 'marry',\n",
       " 'famous',\n",
       " 'declare',\n",
       " 'meet',\n",
       " 'truely',\n",
       " 'verry',\n",
       " 'achieved',\n",
       " 'trouble',\n",
       " 'seen',\n",
       " 'others',\n",
       " 'yep',\n",
       " 'hear',\n",
       " 'playing',\n",
       " 'flawed',\n",
       " 'yess',\n",
       " 'major',\n",
       " 'missing',\n",
       " 'awake',\n",
       " 'ihatequotes',\n",
       " 'pouring',\n",
       " 'upon',\n",
       " 'goal',\n",
       " 'set',\n",
       " 'commit',\n",
       " 'dumb',\n",
       " 'saying',\n",
       " 'sir',\n",
       " 'smoking',\n",
       " 'lets',\n",
       " 'wa',\n",
       " 'dailytweet',\n",
       " 'wht',\n",
       " 'smlllng',\n",
       " 'alhamdulillah',\n",
       " 'bieberlighters',\n",
       " 'week',\n",
       " 'awh',\n",
       " 'cycle',\n",
       " 'upside',\n",
       " 'surprisingly',\n",
       " 'lifestyle',\n",
       " 'accepted',\n",
       " 'hahahahaha',\n",
       " 'minute',\n",
       " 'hello',\n",
       " 'choose',\n",
       " 'earth',\n",
       " 'possibly',\n",
       " 'ahahaha',\n",
       " 'mirror',\n",
       " 'proves',\n",
       " 'deep',\n",
       " 'mann',\n",
       " 'smoke',\n",
       " 'normal',\n",
       " 'close',\n",
       " 'stand',\n",
       " 'sit',\n",
       " 'supposed',\n",
       " 'pretending',\n",
       " 'cute',\n",
       " 'choices',\n",
       " 'liking',\n",
       " 'freddy',\n",
       " 'asap',\n",
       " 'came',\n",
       " 'constantly',\n",
       " 'drink',\n",
       " 'follow',\n",
       " 'negatives',\n",
       " 'looks',\n",
       " 'straight',\n",
       " 'cool',\n",
       " 'soon',\n",
       " 'naa',\n",
       " 'tweeting',\n",
       " 'win',\n",
       " 'piersmorgan',\n",
       " 'aww',\n",
       " 'lose',\n",
       " 'tomorrow',\n",
       " 'badly',\n",
       " 'hang',\n",
       " 'pissed',\n",
       " 'wbu',\n",
       " 'therefore',\n",
       " 'nothin',\n",
       " 'conclusion',\n",
       " 'ffs',\n",
       " 'lool',\n",
       " 'enemies',\n",
       " 'hashtag',\n",
       " 'tweets',\n",
       " 'pls',\n",
       " 'party',\n",
       " 'reievantquotes',\n",
       " 'situations',\n",
       " 'curl',\n",
       " 'ball',\n",
       " 'word',\n",
       " 'dog',\n",
       " 'losing',\n",
       " 'goodness',\n",
       " 'pointless',\n",
       " 'amin',\n",
       " 'excitement',\n",
       " 'tglicks',\n",
       " 'millipede',\n",
       " 'blame',\n",
       " 'pitch',\n",
       " 'problem',\n",
       " 'keeping',\n",
       " 'slut',\n",
       " 'allowed',\n",
       " 'lowkey',\n",
       " 'definitely',\n",
       " 'worried',\n",
       " 'ruin',\n",
       " 'girls',\n",
       " 'resolution',\n",
       " 'forreal',\n",
       " 'liam',\n",
       " 'eventually',\n",
       " 'rest',\n",
       " 'libras',\n",
       " 'libra',\n",
       " 'revenge',\n",
       " 'lovinglife',\n",
       " 'laying',\n",
       " 'filled',\n",
       " 'excellent',\n",
       " 'sohappy',\n",
       " 'cold',\n",
       " 'death',\n",
       " 'took',\n",
       " 'graveyard',\n",
       " 'letting',\n",
       " 'waste',\n",
       " 'space',\n",
       " 'exhausted',\n",
       " 'teenager',\n",
       " 'gave',\n",
       " 'fans',\n",
       " 'intentions',\n",
       " 'comes',\n",
       " 'juss',\n",
       " 'headed',\n",
       " 'incredible',\n",
       " 'involved',\n",
       " 'truth',\n",
       " 'bruh',\n",
       " 'realest',\n",
       " 'ull',\n",
       " 'anybody',\n",
       " 'cinnamon',\n",
       " 'bet',\n",
       " 'double',\n",
       " 'talking',\n",
       " 'bs',\n",
       " 'health',\n",
       " 'imperfect',\n",
       " 'swings',\n",
       " 'emptiness',\n",
       " 'ten',\n",
       " 'gym',\n",
       " 'months',\n",
       " 'suppose',\n",
       " 'l',\n",
       " 'decided',\n",
       " 'sensitive',\n",
       " 'lay',\n",
       " 'noww',\n",
       " 'greatful',\n",
       " 'la',\n",
       " 'perhaps',\n",
       " 'fuq',\n",
       " 'told',\n",
       " 'human',\n",
       " 'asking',\n",
       " 'relive',\n",
       " 'moves',\n",
       " 'completed',\n",
       " 'sobs',\n",
       " 'tha',\n",
       " 'diinawilliams',\n",
       " 'bodoh',\n",
       " 'lifele',\n",
       " 'game',\n",
       " 'harry',\n",
       " 'immediately',\n",
       " 'universe',\n",
       " 'rolling',\n",
       " 'chappy',\n",
       " 'loved',\n",
       " 'fight',\n",
       " 'ahh',\n",
       " 'max',\n",
       " 'grace',\n",
       " 'trip',\n",
       " 'especially',\n",
       " 'np',\n",
       " 'forget',\n",
       " 'sneezed',\n",
       " 'h',\n",
       " 'suicidal',\n",
       " 'mission',\n",
       " 'sware',\n",
       " 'father',\n",
       " 'fa',\n",
       " 'af',\n",
       " 'faith',\n",
       " 'imtrinasalonga',\n",
       " 'taken',\n",
       " 'shitter',\n",
       " 'hehe',\n",
       " 'lls',\n",
       " 'tht',\n",
       " 'ew',\n",
       " 'account',\n",
       " 'built',\n",
       " 'vegas',\n",
       " 'weekends',\n",
       " 'safe',\n",
       " 'negativity',\n",
       " 'watch',\n",
       " 'surrounded',\n",
       " 'obsessed',\n",
       " 'tonite',\n",
       " 'forward',\n",
       " 'followed',\n",
       " 'teacher',\n",
       " 'spoiling',\n",
       " 'hit',\n",
       " 'lazy',\n",
       " 'constant',\n",
       " 'thtblondebitchx',\n",
       " 'control',\n",
       " 'idek',\n",
       " 'cook',\n",
       " 'dick',\n",
       " 'count',\n",
       " 'crap',\n",
       " 'lady',\n",
       " 'whenever',\n",
       " 'hatemylife',\n",
       " 'successful',\n",
       " 'move',\n",
       " 'absolute',\n",
       " ...]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we're going to build a quick function that will find these top 3,000 words \n",
    "# in our positive and negative documents, marking their presence as either positive or negative:\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do this for all of our documents, saving the feature existence booleans \n",
    "# and their respective positive or negative categories by doing:\n",
    "\n",
    "features_f = f.apply(find_features)\n",
    "\n",
    "# Convert the list of dictionaries to dataframe: \n",
    "\n",
    "features_new = pd.DataFrame.from_dict(list(features_f), orient=\"columns\")\n",
    "\n",
    "# Create the column Target in the new dataframe:\n",
    "\n",
    "features_new['Target'] = phrases['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>life</th>\n",
       "      <th>rt</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "      <th>love</th>\n",
       "      <th>hate</th>\n",
       "      <th>want</th>\n",
       "      <th>god</th>\n",
       "      <th>blessed</th>\n",
       "      <th>feel</th>\n",
       "      <th>...</th>\n",
       "      <th>cain</th>\n",
       "      <th>nooo</th>\n",
       "      <th>wio</th>\n",
       "      <th>hunna</th>\n",
       "      <th>contradict</th>\n",
       "      <th>elf</th>\n",
       "      <th>tonight</th>\n",
       "      <th>contemplate</th>\n",
       "      <th>linddsey</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11429</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11430</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11431</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11432</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11433</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11434 rows × 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        life     rt   good  happy   love   hate   want    god  blessed   feel  \\\n",
       "0       True  False  False  False  False   True  False  False    False  False   \n",
       "1      False  False   True  False  False  False  False  False    False  False   \n",
       "2       True  False  False  False  False  False  False  False    False  False   \n",
       "3      False   True  False  False  False  False   True  False    False  False   \n",
       "4       True  False  False  False  False  False  False  False    False  False   \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...      ...    ...   \n",
       "11429  False  False  False  False  False  False  False  False    False  False   \n",
       "11430   True   True  False  False  False  False  False  False    False  False   \n",
       "11431   True  False  False  False  False  False  False  False    False  False   \n",
       "11432  False  False  False  False  False   True  False  False    False  False   \n",
       "11433  False  False  False  False  False  False  False  False    False   True   \n",
       "\n",
       "       ...   cain   nooo    wio  hunna  contradict    elf  tonight  \\\n",
       "0      ...  False  False  False  False       False  False    False   \n",
       "1      ...  False  False  False  False       False  False    False   \n",
       "2      ...  False  False  False  False       False  False    False   \n",
       "3      ...  False  False  False  False       False  False    False   \n",
       "4      ...  False  False  False  False       False  False    False   \n",
       "...    ...    ...    ...    ...    ...         ...    ...      ...   \n",
       "11429  ...  False  False  False  False       False  False    False   \n",
       "11430  ...  False  False  False  False       False  False    False   \n",
       "11431  ...  False  False  False  False       False  False    False   \n",
       "11432  ...  False  False  False  False       False  False    False   \n",
       "11433  ...  False  False  False  False       False  False    False   \n",
       "\n",
       "       contemplate  linddsey  Target  \n",
       "0            False     False       0  \n",
       "1            False     False       1  \n",
       "2            False     False       1  \n",
       "3            False     False       1  \n",
       "4            False     False       0  \n",
       "...            ...       ...     ...  \n",
       "11429        False     False       0  \n",
       "11430        False     False       0  \n",
       "11431        False     False       0  \n",
       "11432        False     False       0  \n",
       "11433        False     False       0  \n",
       "\n",
       "[11434 rows x 3001 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the x and y. Also the training and testing set:\n",
    "\n",
    "X = features_new.drop(columns=\"Target\")\n",
    "y = features_new[\"Target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8302320697200938"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9399490916208144"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfclass = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "rfclass.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9844757844101891"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfclass.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Random Forest Classifier in pickle:\n",
    "\n",
    "rfclass_model = 'rfclass_model.sav'\n",
    "pickle.dump(rfclass, open(rfclass_model, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9379099256668124\n"
     ]
    }
   ],
   "source": [
    "# Load the model:\n",
    "\n",
    "loaded_rfclass = pickle.load(open(rfclass_model, 'rb'))\n",
    "result_rfclass = loaded_rfclass.score(X_test, y_test)\n",
    "print(result_rfclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.93\n",
      "Accuracy of K-NN classifier on test set: 0.91\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 7\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors)\n",
    "knn.fit(X_train, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'.format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'.format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pickle knn:\n",
    "\n",
    "knn_model = 'knn_model.sav'\n",
    "pickle.dump(knn, open(knn_model, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9134236991692173\n"
     ]
    }
   ],
   "source": [
    "loaded_knn = pickle.load(open(knn_model, 'rb'))\n",
    "result_knn = loaded_knn.score(X_test, y_test)\n",
    "print(result_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a module for Sentiment Analysis with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_classifier = VoteClassifier(result_rfclass, result_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "KNN Classifier guardarlo en pickle y cargarlo.\n",
    "transformar texto de la misma forma que los del resto del corpus. funcion process.\n",
    "\n",
    "def clean_up(tweet):\n",
    "    tweet = remove_numbers(tweet)\n",
    "    tweet = remove_special_char(tweet)\n",
    "    return tweet.lower().strip()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sentiment(text):\n",
    "    #hacer preprocessing aqui dentro\n",
    "    clean_text = word_tokenize(text.lower())\n",
    "    feats = find_features(clean_text)\n",
    "    return voted_classifier.classify(feats),voted_classifier.confidence(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentiment_mod'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-277-095ec3cb728d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msentiment_mod\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentiment_mod'"
     ]
    }
   ],
   "source": [
    "import sentiment_mod as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'classify'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-279-17a4f223275d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"My life's horrible\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-276-6280b5d5a427>\u001b[0m in \u001b[0;36msentiment\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mclean_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvoted_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvoted_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-274-4654969d809a>\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mvotes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_classifiers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mvotes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvotes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'classify'"
     ]
    }
   ],
   "source": [
    "# With that, we can now use this file, and the sentiment function as a module. \n",
    "# Here's an example script that might utilize the module:\n",
    "\n",
    "\n",
    "print(sentiment(\"My life's horrible\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
